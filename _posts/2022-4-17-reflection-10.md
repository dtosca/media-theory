---
layout: post
title: ethics and autonomous vehicles
date: 2022-04-11
author: Diana Tosca
comments: true
permalink: /week-10-reflection/
---

I went to an [HCI conference](https://hci4safety.offis.de/) a few years ago, where one of the talks focused on autonomous vehicles. At the time, I was conducting research on the future of work and autonomous vehicle user interfaces, specifically in [level three automation](https://www.jdpower.com/cars/shopping-guides/levels-of-autonomous-driving-explained#:~:text=Level%203%20is%20known%20as,can%20engage%20in%20other%20activities.) (a.k.a. conditional driving automation). In this mode of automation, the car can drive itself in certain conditions (e.g. on a highway with defined roads and clear, moving traffic), and the driver is free to engage in other tasks, while still being prepared to take over driving when needed (known as take over requests). During this talk, of course, the presenter brought up the trolley problem. What should/could an autonomous vehicle do if it’s faced with the choice of killing one person or five people? How do we program for such a scenario? As computer scientists and engineers do, we debated about the trolley problem for a quite a while before moving on. Afterwards, one of the people I’d met at the conference, who was not a computer scientist, talked to me about how very upset she was  with the talk and the trolley problem way of thinking. She insisted that there must be a way to engineer our way around this problem, why does it have to be that the vehicle always ends up killing someone, there must be some way out of it! And that got me thinking, why does the scientific community hyperfixate on this one particular ethical problem? I hadn’t questioned it myself, since I had seen it so many times when discussing autonomous vehicles, it was a no-brainer to me. But here was someone on the outside asking why, and I didn’t (and still don’t) have a good answer. I thought that maybe it just came down to a different way of thinking about autonomous vehicles, maybe it was just a thought experiment to the presenter and not a real world, ethical, moral dilemma that a vehicle might genuinely have to face.

This article. [The folly of trolleys: Ethical challenges and autonomous vehicles](https://www.brookings.edu/research/the-folly-of-trolleys-ethical-challenges-and-autonomous-vehicles/), argues the fallacy and folly of the trolley problem:

>Once we understand that the car is not a human, and that the decision is not a single-shot, black and white one, but one that will be made at the intersection of multiple overlapping probability distributions, we will see that “judgments” about what courses of action to take are going to be not only computationally difficult, but highly context dependent and, perhaps, unknowable by a human engineer a priori.

What I found compelling about this argument is that she makes the distinction between the technology being moral agents:

>We need to take special care to see the asymmetries between cases like the Trolley Problem and algorithms that are not moral agents but make morally important decisions.

We can’t expect an algorithm to know what the right answer to this moral problem is when we ourselves do not know the answer, and the technology is not operating under the same logic as we are (because it can’t).

Tying into my previous work with self-driving cars, in Peter-Paul Verbeek’s talk on “Moralizing Technology and the ethics of things,” I wonder about how our ethics and morals as designers are imbued into autonomous vehicles. The project I worked on focused on autonomous vehicles as commuting assistants - and I had a lot of qualms with this framing of the project. We were researching the UX/UI of autonomous technology to serve the white collar workers, and instilling into it the values of working off the clock. In this way, I think we were playing into societies of control - you don’t *have* to work while you commute to work, but the way this interface is designed, it’d be very convenient to work on a powerpoint presentation while commuting to work. If the meeting is at 10 a.m., and your commute is an hour, your boss could easily tell you to work on the presentation while commuting, thereby erasing the sliver of sanctity the commute has; you’ve made the commute into another surveillance agent. The freedom of doing other tasks while your car drives becomes an obligation to work more. I found it to be very dystopian, and I eventually argued towards pivoting to [researching automation in different areas](https://dl.acm.org/doi/10.1145/3409118.3475154). In response to [question 41](https://medium.com/@frailestthing/do-artifacts-have-ethics-672246c6696a) in this case, I didn’t want to work on a technology that empowered workers to work *more*, that’s unethical. :^)